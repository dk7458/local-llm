# Local LLM Stack

This application deploys multiple language models (e.g., Ollama, llama.cpp) on TrueNAS SCALE using minimal GPU/CPU resources.
